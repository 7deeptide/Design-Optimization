{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "We must construct an objective function for the following problem, handling constraints using the exterior penalty function method.\n",
    "\n",
    "$$\\mbox{min} \\hspace{2 mm} f(x) = (x_1 -100)^2 + (x_2 -50)^2$$\n",
    "\n",
    "$$\\mbox{Subject to } \\hspace{2 mm} g(x) = 170 -x_1 -x_2 \\leq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations =  100001\n",
      "x* =  [109.60332810927328, 59.603328109273427]\n",
      "f(x*) =  184.447821549\n"
     ]
    }
   ],
   "source": [
    "#Steepest descent alorithem\n",
    "#-Erin Schmidt\n",
    "\n",
    "import numpy as np\n",
    "import math as m\n",
    "\n",
    "class eq_int:\n",
    "    def func(alpha, x, norm_del_f, rp, count, n=0): #the objective function\n",
    "        count += 1\n",
    "        x[0] += alpha*norm_del_f[0]\n",
    "        x[1] += alpha*norm_del_f[1]\n",
    "        f = st_dec.func(x, rp, n)[0]\n",
    "        return f, count\n",
    "    \n",
    "    def mini(au, al, x, norm_del_f, rp, count): #evaluates f at the minimum (or optimum) stationary point\n",
    "        alpha = (au + al)*0.5\n",
    "        (f, count) = eq_int.func(alpha, x, norm_del_f, rp, count)\n",
    "        return f, alpha, count\n",
    "    \n",
    "    def search(al, x, norm_del_f, rp, delta=0.01, epsilon=1E-4, count=0):\n",
    "        (f, count) = eq_int.func(al, x, norm_del_f, rp, count)\n",
    "        fl = f #function value at lower bound\n",
    "\n",
    "        while True:\n",
    "            aa = delta\n",
    "            (f, count) = eq_int.func(aa, x, norm_del_f, rp, count)\n",
    "            fa = f\n",
    "            if fa > fl:\n",
    "                delta = delta * 0.1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "            au = aa + delta\n",
    "            (f, count) = eq_int.func(au, x, norm_del_f, rp, count)\n",
    "            fu = f\n",
    "            if fa > fu:\n",
    "                al = aa\n",
    "                aa = au\n",
    "                fl = fa\n",
    "                fa = fu\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "            if (au - al) > epsilon: #compares interval size to convergence criteria\n",
    "                delta = delta * 0.1\n",
    "                aa = al #intermediate alpha\n",
    "                fa = fl #intermediate alpha function value\n",
    "                while True:\n",
    "                    au = aa + delta\n",
    "                    (f, count) = eq_int.func(au, x, norm_del_f, rp, count)\n",
    "                    fu = f\n",
    "                    if fa > fu: \n",
    "                        al = aa\n",
    "                        aa = au\n",
    "                        fl = fa\n",
    "                        fa = fu\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                continue\n",
    "            else:\n",
    "                (f, alpha, count) = eq_int.mini(au, al, x, norm_del_f, rp, count)\n",
    "                return f, alpha, count\n",
    "       \n",
    "class st_dec:\n",
    "    def func(x, rp, n=0):\n",
    "        g = max([0, 170 - x[0] -x[1]])\n",
    "        f = (x[0] - 100)**2 + (x[1] - 50)**2 + rp*g**2 #pseudo-objective value at x\n",
    "        del_f = [2*(x[0] - 100) + rp*(-2*g), 2*(x[1] -50) + rp*(-2*g)] #gradient value at x\n",
    "        del_f = np.array(del_f)\n",
    "        f_val = (x[0] - 100)**2 + (x[1] - 50)**2 #actual function value\n",
    "        n += 1\n",
    "        return f, del_f, n, f_val\n",
    "    \n",
    "    def steepest(x, rp):\n",
    "        n = 0 #iteration counter\n",
    "        alpha = .01 #initial step size\n",
    "        (f, del_f, n, f_val) = st_dec.func(x, rp, n)\n",
    "        while True:\n",
    "            x_old = x\n",
    "            alpha_old = alpha\n",
    "            norm_del_f = -del_f/m.sqrt(del_f[0]**2+del_f[1]**2) #normalize grad vector\n",
    "            alpha = eq_int.search(alpha, x, norm_del_f, rp)[1]\n",
    "            x[0] += alpha*norm_del_f[0] #next step x-values\n",
    "            x[1] += alpha*norm_del_f[1]  \n",
    "            (f, del_f, n, f_val) = st_dec.func(x, rp, n)\n",
    "           \n",
    "            # Convergence criteria\n",
    "            #if abs((alpha - alpha_old)/alpha) < 1E-12: #convergence criteria\n",
    "             #   return f, n, x, alpha, f_val\n",
    "                \n",
    "            #a=np.array(x).reshape((2,1))\n",
    "            #b=np.array(x_old).reshape((2,1))\n",
    "            #if np.linalg.norm(b - a) < 1E-6:\n",
    "             #   return f, n, x, alpha, f_val\n",
    "                \n",
    "            if n > 100000:\n",
    "                return f, n, x, alpha, f_val\n",
    "\n",
    "\n",
    "#starting design        \n",
    "x = [0,0]\n",
    "rp = 100\n",
    "(f, n, x, alpha, f_val) = st_dec.steepest(x, rp)\n",
    "print('iterations = ', n)\n",
    "print('x* = ', x)\n",
    "print('f(x*) = ', f_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "For the sake of comparison I experimented with an alternate approach, without using an equal interval search to optimize the step-size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations =  16318\n",
      "x* =  [ 109.9747016   59.9747016]\n",
      "f(x*) =  198.98934383\n"
     ]
    }
   ],
   "source": [
    "# Another, simpler approach, without dynamic step sizes\n",
    "import numpy as np\n",
    "\n",
    "# Initial Guess\n",
    "x_old = np.array([0, 0]).reshape((2,1))\n",
    "x_new = np.array([10,100]).reshape((2,1))\n",
    "\n",
    "# Parameters\n",
    "gamma = .01 # Step size\n",
    "epsilon = 1E-3 # Convergence parameter\n",
    "\n",
    "#Objective function\n",
    "def func(x, rp=200):\n",
    "    g = max([0, 170 - x[0] - x[1]])\n",
    "    f = (x[0] - 100)**2 + (x[1] - 50)**2 + rp*g**2 #pseudo-objective value at x\n",
    "    del_f = [2*(x[0] - 100) + rp*(-2*g), 2*(x[1] -50) + rp*(-2*g)] #gradient value at x\n",
    "    del_f = np.array(del_f).reshape((2,1))\n",
    "    f_val = (x[0] - 100)**2 + (x[1] - 50)**2 #actual function value\n",
    "    return f, del_f, f_val, g\n",
    "\n",
    "# Steepest Descent\n",
    "i = 0\n",
    "while np.linalg.norm(x_old - x_new) > epsilon:\n",
    "    x_old = x_new\n",
    "    x_new = x_old - gamma * func(x_old)[1]\n",
    "    i = i + 1\n",
    "\n",
    "print('iterations = ', i)\n",
    "print('x* = ', x_new.T[0])\n",
    "print('f(x*) = ', func(x_new)[2][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the steepest descent scheme runs much faster than the one using the dynamic step-size optimization (2 orders of magnitude less physical time for the same number of iterations).  Various values for the inital guess seems to find a similar optimum point, however the number of iterations required to find it can vary substantially. This applies also to the step size and the convergence parameter. The value of the penalty function coefficient does have a strong effect on the final optimum $x^{*}$. Too large values of `rp` can cause the algorithem to fail to converge. Too small values of `rp` and the penalty function fails to be optimized. \n",
    "\n",
    "Regardless both versions of the steepest descent algorithem come close to the 'cannonical' value of $f(x^*) = 200$ at $x^* = [110, 60]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
